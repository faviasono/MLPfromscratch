# MultiLayerPerceptron
During the Data Mining course at Eindhoven University I had to implement a MLP from scratch to perform a binary classification problem.
The architecture used is a 2-hidden layer network with 10 neurons each layer. For the hidden layers I used the Relu activation function and 
for the output layer a simgoid function to compute a probability distribution. 
A detailed analysis of hyperparameters and initializations used is present in the report.ipynb

